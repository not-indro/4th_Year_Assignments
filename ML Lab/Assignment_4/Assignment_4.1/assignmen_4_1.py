# -*- coding: utf-8 -*-
"""Assignmen_4.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yy66Nwh4vgL73ECA7XJXWwblCCMN_i_J

##**Name - Indrnail bain**
##**Enrollment No. - 2020CSB039**
##**Assignment - 4 ( TITANIC Dataset)**

**1. Download Titanic Dataset (https://www.kaggle.com/heptapod/titanic/version/1#) and do initial pre-processing including normalization, na or zero column handling, train test split, and others (Write an explanation of each in the report).**
"""

from google.colab import drive
drive.mount('/content/drive')

BASE_PATH = "/content/drive/MyDrive/CSV Files - COLAB/train_and_test2.csv"

import pandas as pd
titanic_df = pd.read_csv(BASE_PATH)
titanic_df

titanic_df.dropna()

titanic_df.columns

titanic_df = titanic_df[
    filter(
lambda colName: "zero" not in colName,
        titanic_df.columns
    )
]
titanic_df = titanic_df.drop("Passengerid", axis=1)
titanic_df

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

def one_hot_encode(X: pd.DataFrame, col_name: str) -> pd.DataFrame:
    """
    Alters X by one-hot-encoding the values of the col_name using OneHotEncoder(),
    returns the altered DataFrame
    """
    encoder = OneHotEncoder()
    encoded_df = pd.DataFrame(
        encoder.fit_transform(X[[col_name]]).toarray(),
        index=X.index,
        columns=encoder.get_feature_names_out([col_name])
    )
    X = X.join(encoded_df)
    X = X.drop(col_name, axis=1)
    return X

def standardize(df: pd.DataFrame, col_name: str) -> pd.DataFrame:
    """
    Alters df by standardizing the values of the col_name using StandardScaler(),
    returns the altered DataFrame
    """
    scaler = StandardScaler()
    df[[col_name]] = pd.DataFrame(
        data=scaler.fit_transform(df[[col_name]]),
        index=df.index,
        columns=[col_name]
    )
    return df

# Pclass has value ranging from 0 to 3 (doing OneHotEncoding)
# Sex has value ranging from 0 to 2 (doing OneHotEncoding)
# Embarked has value ranging from 0 to 3 (doing OneHotEncoding)
columns_to_encode = ["Pclass", "Embarked", "Sex"]

for column in columns_to_encode:
  titanic_df = one_hot_encode(titanic_df, column)
titanic_df

# Age, Fare, sibsp, Parch needs to be standardized as their values
# are not in the range of 0 to 1
columns_to_standardize = ['Age', "Fare", 'sibsp', "Parch"]

for column in columns_to_standardize:
    titanic_df = standardize(titanic_df, column)
titanic_df

"""**2. Train the SVM using the below kernels with parameters, present the support vectors in the table of the comparison of the model along with accuracy.**

* Linear
* Polynomial: where degree d is set to 2, 3 and 5
* RBF
* Sigmoid


"""

from sklearn.svm import SVC
import pandas as pd

def trainSVC(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.DataFrame,
    y_test: pd.DataFrame,
    kernel: str,
    degree: int = 3,
    return_model: bool = False
):
    """
    degree is ignored if kernel is not 'poly'
    """
    model = SVC(kernel=kernel, degree=degree)
    model.fit(X_train, y_train.iloc[:,0])
    accuracy = model.score(X_test, y_test)

    if return_model:
        return model

    return [kernel, degree if kernel=='poly' else 'None', accuracy, model.support_vectors_]

from sklearn.model_selection import train_test_split
X = titanic_df.drop('2urvived', axis=1)
y = titanic_df[['2urvived']]
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)
trainSVC(X_train, X_test, y_train, y_test, 'linear')

configs = [
    ['linear'],
    ['poly', 2],
    ['poly', 3],
    ['poly', 5],
    ['rbf'],
    ['sigmoid']
]
data = [
trainSVC(X_train, X_test, y_train, y_test, *config) for config in configs
]
pd.DataFrame(data, columns=['kernel', 'degree (only for poly)', 'accuracy','support vectors'])

"""**3. Take only two features from the dataset and train the models with the same parameters and plot the graphs to show the boundaries. Also, create a custom kernel function of your own using a mathematical function for suggestion Lograthmic or Tangent function.**"""

from sklearn.inspection import DecisionBoundaryDisplay
import matplotlib.pyplot as plt

def plot_decision_boundary(kernel: str, degree=1):
    X = titanic_df.iloc[:, 0:2]
    model = SVC(kernel=kernel, degree=degree).fit(X, y.iloc[:, 0])
    disp = DecisionBoundaryDisplay.from_estimator(model, X, alpha=0.5, eps=0.1)
    disp.ax_.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y.iloc[:, 0])

    if kernel == 'poly':
        plt.title(f"{kernel} {degree}")
    else:
        plt.title(f"{kernel}")

    plt.show()

configs = [("linear",), ("poly", 3), ("rbf",)]
for config in configs:
    plot_decision_boundary(*config)

import numpy as np

def my_kernel(X:"np.array", Y: "np.array") -> "np.array":
  M = np.array([[1, 0], [0, 1]])
  return np.dot(np.dot(np.tan(X), M), Y.T)
X_sub = titanic_df.iloc[:, 0:2]
model = SVC(kernel=my_kernel).fit(X_sub, y.iloc[:, 0])
disp = DecisionBoundaryDisplay.from_estimator(model, X_sub, alpha=0.5, eps=0.1)
disp.ax_.scatter(X_sub.iloc[:, 0], X_sub.iloc[:, 1], c=y.iloc[:, 0])
plt.title('Tan Kernel')
plt.show()

"""**4. For RBF kernel vary the control parameter C with a binary search technique to reach an optimal C value. Plot the graph for validation accuracy. Using this, mention the situation of overfitting and underfitting. Set Gamma to 0.5. Create a function for the whole process. [Maximum 20 runs]**"""

def binary_search_C(X_train: "pd.DataFrame", X_val: "pd.DataFrame", y_train: "pd.DataFrame", y_val: "pd.DataFrame"):
    right = 0.1
    left = 0
    max_acc = 0
    acc = 0.1
    accuracies = []
    Cs = []
    mid = 0  # Initialize mid

    while acc >= max_acc:
        left = right
        right *= 2
        model = SVC(kernel='rbf', gamma=0.5, C=right).fit(X_train, y_train.iloc[:, 0])
        max_acc = max(max_acc, acc)
        acc = model.score(X_val, y_val)
        print(left, right, acc)
        accuracies.append(acc)
        Cs.append(right)

    plt.plot(Cs, accuracies, 'o-', label="Forward Exponentiation")
    Cs = []
    accuracies = []
    print("phase2")

    while left <= right:
        mid = (left + right) / 2
        model = SVC(kernel='rbf', C=mid, gamma=0.5).fit(X_train, y_train.iloc[:, 0])
        acc = model.score(X_val, y_val)
        accuracies.append(acc)
        Cs.append(mid)
        print(left, mid, right, acc)
        if acc < max_acc:
            right = mid - 0.0001
        elif acc > max_acc:
            left = mid + 0.0001
            max_acc = acc
        else:
            break

    plt.plot(Cs, accuracies, 'o-', label="Binary Search")
    plt.title("Exponentiation Propagation and Binary Search to find the best C")
    plt.xlabel("C")
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    return mid

X = titanic_df.iloc[:, 0:2]
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
best_c = binary_search_C(X_train, X_val, y_train, y_val)

"""**5. Using the above-created function now varies the Gamma parameter with the same binary search techniques as above for the C value which has maximum validation accuracy. Explain, whether the above calculated maximum test accuracy is the optimal test accuracy or there can be a better value of C and Gamma.**"""

def binary_search_gamma(X_train: "pd.DataFrame", X_val: "pd.DataFrame", y_train: "pd.DataFrame", y_val: "pd.DataFrame", best_c: "float"):
    right = 0.1
    left = 0
    max_acc = 0
    acc = 0.1
    accuracies = []
    Cs = []

    while acc >= max_acc:
        left = right
        right *= 2
        model = SVC(kernel='rbf', gamma=right, C=best_c).fit(X_train, y_train.iloc[:, 0])
        max_acc = max(max_acc, acc)
        acc = model.score(X_val, y_val)
        print(left, right, acc)
        accuracies.append(acc)
        Cs.append(right)

    plt.plot(Cs, accuracies, 'o-', label='Forward Exponentiation')
    Cs = []
    accuracies = []
    print("phase2")

    while left <= right:
        mid = (left + right) / 2
        model = SVC(kernel='rbf', C=best_c, gamma=mid).fit(X_train, y_train.iloc[:, 0])
        acc = model.score(X_val, y_val)
        accuracies.append(acc)
        Cs.append(mid)
        print(left, mid, right, acc)
        if acc < max_acc:
            right = mid - 0.0001
        elif acc > max_acc:
            left = mid + 0.0001
            max_acc = acc
        else:
            break

    plt.plot(Cs, accuracies, 'o-', label='Binary Search')
    plt.title("Exponentiation Propagation and Binary Search to find the best gamma")
    plt.xlabel("gamma")
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    return mid

binary_search_gamma(X_train, X_val, y_train, y_val, best_c)